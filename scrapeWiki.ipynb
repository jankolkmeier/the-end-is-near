{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_url = 'https://de.wikipedia.org'\n",
    "alpha = [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"Sch\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land = 'Baden-Württemberg'\n",
    "url = wiki_url + list_url + urllib.parse.quote(land)\n",
    "website = requests.get(url).text\n",
    "soup = BeautifulSoup(website,'lxml')\n",
    "\n",
    "toc = soup.find('div',{'class':'toc'})\n",
    "alpha_urls = []\n",
    "for link in toc.findAll('a'):\n",
    "    alpha_urls.append(link.get('href'))\n",
    "\n",
    "for alpha_url in alpha_urls:\n",
    "    char_url = wiki_url + alpha_url\n",
    "    char_website = requests.get(char_url).text\n",
    "    char_soup = BeautifulSoup(char_website,'lxml')\n",
    "    \n",
    "    for li in char_soup.select(\"#bodyContent table li\"):\n",
    "        list = li.findAll(\"a\")\n",
    "        str = list[0].get('title')\n",
    "        str = str + \"\\t\"\n",
    "        if len(list) > 1:\n",
    "            str = str + li.findAll('a')[1].get('title')\n",
    "        str = str + \"\\t\"\n",
    "        str = str + \"Baden-Würtemberg\"\n",
    "        print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land = 'Bayern'\n",
    "website = requests.get(\"https://de.wikipedia.org/wiki/Vorlage:Navigationsleiste_Orte_in_Bayern\").text\n",
    "soup = BeautifulSoup(website,'lxml')\n",
    "\n",
    "kreise_urls = []\n",
    "for link in soup.select(\".NavContent a\"):\n",
    "    kreise_urls.append(link.get(\"href\"))\n",
    "\n",
    "for link in kreise_urls:\n",
    "    kreis_url = wiki_url + link\n",
    "    kreis_website = requests.get(kreis_url).text\n",
    "    kreis_soup = BeautifulSoup(kreis_website,'lxml')\n",
    "    for table in kreis_soup.select(\"table\"):\n",
    "        h2 = table.select(\"h2 span.mw-headline\")\n",
    "        if len(h2) > 0:\n",
    "            i = 0\n",
    "            singleChar = False\n",
    "            while i < 3:\n",
    "                if len(h2[i].get(\"id\")) == 1:\n",
    "                    singleChar = True\n",
    "                    break;\n",
    "                i += 1\n",
    "            if(singleChar):\n",
    "                for li in table.select(\"li\"):\n",
    "                    link = li.find(\"a\")\n",
    "                    str =  link.contents[0]\n",
    "                    str = str + \"\\t\"\n",
    "                    \n",
    "                    if len(link.next.next) > 1:\n",
    "                        str = str + link.next.next\n",
    "                    else:\n",
    "                        str = str + link.get('title') #namensgebend\n",
    "            \n",
    "                    str = str + \"\\t\"\n",
    "                    str = str + \"Bayern\"\n",
    "                    print(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land = \"Mecklenburg-Vorpommern\"\n",
    "mv_website = requests.get('https://de.wikipedia.org/wiki/Liste_der_Ortsteile_in_Mecklenburg-Vorpommern').text\n",
    "mv_soup = BeautifulSoup(mv_website,'lxml')\n",
    "for li in mv_soup.select(\"li\"):\n",
    "    link = li.select(\"a\")\n",
    "    str = link[0].contents[0]\n",
    "    str = str + \"\\t\"\n",
    "    str = str + link[1].contents[0]\n",
    "    str = str + \"\\tMecklenburg Vorpommern\"\n",
    "    print(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land = \"Schleswig-Holstein\"\n",
    "mv_website = requests.get('https://de.wikipedia.org/wiki/Liste_der_Ortsteile_in_Schleswig-Holstein').text\n",
    "mv_soup = BeautifulSoup(mv_website,'lxml')\n",
    "for li in mv_soup.select(\"li\"):\n",
    "    link = li.select(\"a\")\n",
    "    str = link[0].contents[0]\n",
    "    str = str + \"\\t\"\n",
    "    str = str + link[1].contents[0]\n",
    "    str = str + \"\\tSchleswig-Holstein\"\n",
    "    print(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land = 'Thüringen'\n",
    "website = requests.get('https://de.wikipedia.org/wiki/Liste_der_Orte_in_Th%C3%BCringen').text\n",
    "soup = BeautifulSoup(website,'lxml')\n",
    "\n",
    "alpha_urls = []\n",
    "for link in soup.select(\"table.wikitable a\"):\n",
    "    alpha_urls.append(link.get('href'))\n",
    "\n",
    "for alpha_url in alpha_urls:\n",
    "    char_url = wiki_url + alpha_url\n",
    "    char_website = requests.get(char_url).text\n",
    "    char_soup = BeautifulSoup(char_website,'lxml')\n",
    "    for row in char_soup.select(\"table\")[1].select(\"tr\"):\n",
    "        cols = row.select(\"td\")\n",
    "        if len(cols) > 0:\n",
    "            str = cols[0].find(text=True)\n",
    "            str = str + \"\\t\" + cols[1].find(text=True)\n",
    "            print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land = 'Nordrhein-Westfalen'\n",
    "website = requests.get('https://de.wikipedia.org/w/index.php?title=Kategorie:Ort_in_Nordrhein-Westfalen').text\n",
    "soup = BeautifulSoup(website,'lxml')\n",
    "\n",
    "for link in soup.select(\"#mw-pages li a\"):\n",
    "    str = link.contents[0]+\"\\t\"+land\n",
    "    print(str)\n",
    "\n",
    "firstLevel = []\n",
    "for h3 in soup.select(\"#mw-subcategories h3\"):\n",
    "    if h3.contents[0] in alpha:\n",
    "        for a in h3.find_next(\"ul\").select(\"li a\"):\n",
    "            firstLevel.append(a.get(\"href\"))\n",
    "\n",
    "for link in firstLevel:\n",
    "    #print(link)\n",
    "    website = requests.get('https://de.wikipedia.org'+link).text\n",
    "    soup = BeautifulSoup(website,'lxml')\n",
    "    \n",
    "    for link in soup.select(\"#mw-pages li a\"):\n",
    "        str = link.contents[0]+\"\\t\"+land\n",
    "        print(str)\n",
    "        \n",
    "    secondLevel = []\n",
    "    for h3 in soup.select(\"#mw-subcategories h3\"):\n",
    "        if h3.contents[0] in alpha:\n",
    "            for a in h3.find_next(\"ul\").select(\"li a\"):\n",
    "                secondLevel.append(a.get(\"href\"))\n",
    "    \n",
    "    for link in secondLevel:\n",
    "        website = requests.get('https://de.wikipedia.org'+link).text\n",
    "        soup = BeautifulSoup(website,'lxml')\n",
    "        for h3 in soup.select(\"#mw-pages h3\"):\n",
    "            if h3.contents[0] in alpha:\n",
    "                for link in h3.find_next(\"ul\").select(\"li a\"):\n",
    "                    str = link.contents[0]+\"\\t\"+land\n",
    "                    print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land = 'Rheinland-Pfalz'\n",
    "website = requests.get('https://de.wikipedia.org/wiki/Kategorie:Ort_in_Rheinland-Pfalz').text\n",
    "soup = BeautifulSoup(website,'lxml')\n",
    "\n",
    "for link in soup.select(\"#mw-pages li a\"):\n",
    "    str = link.contents[0]+\"\\t\"+land\n",
    "    print(str)\n",
    "\n",
    "firstLevel = []\n",
    "alpha = [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"Sch\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"]\n",
    "for h3 in soup.select(\"#mw-subcategories h3\"):\n",
    "    if h3.contents[0] in alpha:\n",
    "        for a in h3.find_next(\"ul\").select(\"li a\"):\n",
    "            firstLevel.append(a.get(\"href\"))\n",
    "\n",
    "for link in firstLevel:\n",
    "    #print(link)\n",
    "    website = requests.get('https://de.wikipedia.org'+link).text\n",
    "    soup = BeautifulSoup(website,'lxml')\n",
    "    \n",
    "    for link in soup.select(\"#mw-pages li a\"):\n",
    "        str = link.contents[0]+\"\\t\"+land\n",
    "        print(str)\n",
    "        \n",
    "    secondLevel = []\n",
    "    for h3 in soup.select(\"#mw-subcategories h3\"):\n",
    "        if h3.contents[0] in alpha:\n",
    "            for a in h3.find_next(\"ul\").select(\"li a\"):\n",
    "                secondLevel.append(a.get(\"href\"))\n",
    "    \n",
    "    for link in secondLevel:\n",
    "        website = requests.get('https://de.wikipedia.org'+link).text\n",
    "        soup = BeautifulSoup(website,'lxml')\n",
    "        for h3 in soup.select(\"#mw-pages h3\"):\n",
    "            if h3.contents[0] in alpha:\n",
    "                for link in h3.find_next(\"ul\").select(\"li a\"):\n",
    "                    str = link.contents[0]+\"\\t\"+land\n",
    "                    print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "land = 'Sachsen-Anhalt'\n",
    "website = requests.get('https://de.wikipedia.org/wiki/Kategorie:Ort_in_Sachsen-Anhalt').text\n",
    "soup = BeautifulSoup(website,'lxml')\n",
    "\n",
    "for link in soup.select(\"#mw-pages li a\"):\n",
    "    str = link.contents[0]+\"\\t\"+land\n",
    "    print(str)\n",
    "\n",
    "firstLevel = []\n",
    "alpha = [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"Sch\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"]\n",
    "for h3 in soup.select(\"#mw-subcategories h3\"):\n",
    "    if h3.contents[0] in alpha:\n",
    "        for a in h3.find_next(\"ul\").select(\"li a\"):\n",
    "            firstLevel.append(a.get(\"href\"))\n",
    "\n",
    "for link in firstLevel:\n",
    "    #print(link)\n",
    "    website = requests.get('https://de.wikipedia.org'+link).text\n",
    "    soup = BeautifulSoup(website,'lxml')\n",
    "    \n",
    "    for link in soup.select(\"#mw-pages li a\"):\n",
    "        str = link.contents[0]+\"\\t\"+land\n",
    "        print(str)\n",
    "        \n",
    "    secondLevel = []\n",
    "    for h3 in soup.select(\"#mw-subcategories h3\"):\n",
    "        if h3.contents[0] in alpha:\n",
    "            for a in h3.find_next(\"ul\").select(\"li a\"):\n",
    "                secondLevel.append(a.get(\"href\"))\n",
    "    \n",
    "    for link in secondLevel:\n",
    "        website = requests.get('https://de.wikipedia.org'+link).text\n",
    "        soup = BeautifulSoup(website,'lxml')\n",
    "        for h3 in soup.select(\"#mw-pages h3\"):\n",
    "            if h3.contents[0] in alpha:\n",
    "                for link in h3.find_next(\"ul\").select(\"li a\"):\n",
    "                    str = link.contents[0]+\"\\t\"+land\n",
    "                    print(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
